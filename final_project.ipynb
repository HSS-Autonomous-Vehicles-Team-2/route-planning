{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import ObjectDetector\n",
    "from jetbot import Camera\n",
    "from jetbot import Robot\n",
    "from jetbot import bgr8_to_jpeg\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# basic setup\n",
    "model = ObjectDetector('ssd_mobilenet_v2_coco.engine')\n",
    "camera = Camera.instance(width=300, height=300, fps=10)\n",
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 300\n",
    "height = 300\n",
    "\n",
    "def detection_center(detection):\n",
    "    \"\"\"Computes the center x, y coordinates of the object\"\"\"\n",
    "    bbox = detection['bbox']\n",
    "    center_x = (bbox[0] + bbox[2]) / 2.0 - 0.5\n",
    "    center_y = (bbox[1] + bbox[3]) / 2.0 - 0.5\n",
    "    return (center_x, center_y)\n",
    "    \n",
    "def norm(vec):\n",
    "    \"\"\"Computes the length of the 2D vector\"\"\"\n",
    "    return np.sqrt(vec[0]**2 + vec[1]**2)\n",
    "\n",
    "def closest_detection(detections):\n",
    "    \"\"\"Finds the detection closest to the image center\"\"\"\n",
    "    closest_detection = None\n",
    "    for det in detections:\n",
    "        center = detection_center(det)\n",
    "        if closest_detection is None:\n",
    "            closest_detection = det\n",
    "        elif norm(detection_center(det)) < norm(detection_center(closest_detection)):\n",
    "            closest_detection = det\n",
    "    return closest_detection\n",
    "\n",
    "def estimate_distance(detection):\n",
    "    focal_length = 154.0  #need to caliborate your camera to get this\n",
    "    obj_info = object_info_dict[detection['label']]\n",
    "    bbox = detection['bbox']  \n",
    "    obj_height = obj_info[1]\n",
    "    obj_height_in_img = width * (bbox[3] - bbox[1])        \n",
    "    dist = obj_height * focal_length / obj_height_in_img\n",
    "    return dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_local_steering_in_progress = False\n",
    "is_moving_to_target_in_progress = False\n",
    "is_locking_target_in_progress = False\n",
    "prev_obstacle_label = -1\n",
    "\n",
    "is_finished_target_locking = True\n",
    "\n",
    "\n",
    "target_obj_tall = 1\n",
    "target_obj_short = 31\n",
    "\n",
    "obstacle_obj_1 = 64\n",
    "obstacle_obj_2 = 18\n",
    "\n",
    "object_info_dict = {1:(\"person\", 65), 31:(\"short_object\", 40), 18:(\"dog\", 6.2), 64:(\"plant\", 12)}\n",
    "\n",
    "def local_steering(detections):\n",
    "    # check if local steering is needed based on distance\n",
    "    global is_local_steering_in_progress\n",
    "    global prev_obstacle_label\n",
    "    global is_moving_to_target_in_progress\n",
    "    is_local_steering_needed = False\n",
    "    \n",
    "    obstacle = None\n",
    "    obstacle_dist = 100000\n",
    "    \n",
    "    center_offset = 0\n",
    "    for det in detections:\n",
    "        bbox = det['bbox']\n",
    "        label = det['label']\n",
    "       \n",
    "        if(label == obstacle_obj_1 or label == obstacle_obj_2):\n",
    "            dist = estimate_distance(det)\n",
    "            print(str(label) + \" \" + str(dist) + \" \" + str(height * bbox[3]))\n",
    "            \n",
    "            is_local_steering_needed = (height * bbox[3] > height - 30)\n",
    "            if(label == obstacle_obj_1):\n",
    "                is_local_steering_needed = dist < 17\n",
    "            elif(label == obstacle_obj_2):\n",
    "                is_local_steering_needed = dist < 17\n",
    "            if(is_local_steering_needed):\n",
    "                obstacle = det\n",
    "                obstacle_dist = dist\n",
    "                \n",
    "                center = detection_center(det)\n",
    "                center_offset = center[0]\n",
    "            break\n",
    "  \n",
    "\n",
    "    if(is_local_steering_needed and obstacle is not None):  \n",
    "        #if seen this before, skip local steering\n",
    "        if(obstacle['label'] == prev_obstacle_label):\n",
    "            return \n",
    "        is_local_steering_in_progress = True\n",
    "        is_moving_to_target_in_progress = False\n",
    "        print(\"start local steering\")\n",
    "        #local_steering code come here\n",
    "        \n",
    "        forward_sleep_time = 1\n",
    "        \n",
    "        if(obstacle['label'] == obstacle_obj_1):\n",
    "            forward_sleep_time = 1.4\n",
    "        elif(obstacle['label'] == obstacle_obj_2):\n",
    "            forward_sleep_time = 1.2\n",
    "            \n",
    "        robot.stop()\n",
    "        time.sleep(2)\n",
    "        \n",
    "        on_center = center_offset <= 0.125 and center_offset >= -0.125\n",
    "            \n",
    "        while(on_center == False):\n",
    "            print(\"center_offset \" + str(center_offset))\n",
    "            if center_offset > 0.125:\n",
    "                print(\"on right\")\n",
    "                robot.right(0.28)\n",
    "                time.sleep(0.1)\n",
    "                robot.stop()\n",
    "                time.sleep(1)\n",
    "            elif center_offset < -0.125:\n",
    "                print(\"on left\")\n",
    "                robot.left(0.28)\n",
    "                time.sleep(0.1)\n",
    "                robot.stop()\n",
    "                time.sleep(1)\n",
    "                    \n",
    "            image = camera.value\n",
    "            detections = model(image)\n",
    "            visualize_detections(image, detections[0]) \n",
    "            for det in detections[0]:\n",
    "                bbox = det['bbox']\n",
    "                label = det['label']\n",
    "                if(label == obstacle_obj_1 or label == obstacle_obj_2):\n",
    "                    center = detection_center(det)\n",
    "                    on_center = center[0] < 0.125 and center[0] > -0.125\n",
    "                    center_offset = center[0]\n",
    "                    break\n",
    "            \n",
    "            \n",
    "        robot.set_motors(0.3, 0.305)\n",
    "        time.sleep(0.2)\n",
    "                \n",
    "        robot.left(speed=0.25)\n",
    "        time.sleep(0.32)\n",
    "        robot.set_motors(0.3, 0.305)\n",
    "        time.sleep(forward_sleep_time)\n",
    "\n",
    "        robot.right(speed=0.25)\n",
    "        time.sleep(0.32)\n",
    "        robot.set_motors(0.3, 0.305)\n",
    "        time.sleep(forward_sleep_time)\n",
    "        \n",
    "#         robot.left(speed=0.25)\n",
    "#         time.sleep(0.28)\n",
    "#         robot.set_motors(0.3, 0.305)\n",
    "#         time.sleep(forward_sleep_time)\n",
    "    \n",
    "        robot.stop()\n",
    "        time.sleep(1.0)\n",
    "        print(\"end local steering\")\n",
    "        prev_obstacle_label = obstacle['label']\n",
    "        is_local_steering_in_progress = False\n",
    "    \n",
    "    \n",
    "def visualize_detections(image, detections):\n",
    "\n",
    "    for det in detections:\n",
    "        bbox = det['bbox']\n",
    "        label = det['label']\n",
    "       \n",
    "        if(label == target_obj_tall or label == target_obj_short):\n",
    "            dist = estimate_distance(det)\n",
    "            cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (0, 0, 255), 2)\n",
    "            cv2.putText(image, object_info_dict[label][0] + \":\" + \"{:.2f}\".format(dist), (int(width * bbox[0]), int(height * bbox[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255),2)\n",
    "        elif(label == obstacle_obj_1 or label == obstacle_obj_2):\n",
    "            dist = estimate_distance(det)\n",
    "            cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (0, 255, 0), 2)\n",
    "            cv2.putText(image, object_info_dict[label][0] + \":\" + \"{:.2f}\".format(dist), (int(width * bbox[0]), int(height * bbox[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0),2)\n",
    "#             cv2.putText(image, str(height * bbox[3]) + \" \" + \":\" + \"{:.2f}\".format(dist) + \" \" + str(height * bbox[3]), (int(width * bbox[0]), int(height * bbox[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0),2)\n",
    "        \n",
    "        else:\n",
    "            cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (255, 0, 0), 2)\n",
    "            cv2.putText(image, str(label), (int(width * bbox[0]), int(height * bbox[1]) + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0),2)\n",
    "    \n",
    "    image_widget.value = bgr8_to_jpeg(image)\n",
    "   \n",
    "    \n",
    "def lock_and_move_to_target(detections):\n",
    "    \n",
    "    \n",
    "    global is_finished_target_locking\n",
    "\n",
    "    global is_locking_target_in_progress\n",
    "    global is_moving_to_target_in_progress\n",
    "    \n",
    "    if(is_locking_target_in_progress == True):\n",
    "        return\n",
    "    \n",
    "    if(is_moving_to_target_in_progress == True):\n",
    "        return\n",
    "    \n",
    "    \n",
    "    is_locking_target_in_progress = True\n",
    "    \n",
    "    \n",
    "    found_object = False\n",
    "    \n",
    "    print(\"start locking target\")\n",
    "    \n",
    "    while(found_object == False):\n",
    "\n",
    "   \n",
    "        is_finished_target_locking = False\n",
    "        is_locking_target_in_progress = True\n",
    "        \n",
    "        image = camera.value\n",
    "    \n",
    "        detections = model(image)\n",
    "        print(\"visualize \")\n",
    "        visualize_detections(image, detections[0])\n",
    "        center_offset = 0\n",
    "        for det in detections[0]:\n",
    "            print(\"detection \")\n",
    "            bbox = det['bbox']\n",
    "            label = det['label']\n",
    "            if(label == target_obj_tall or label == target_obj_short):\n",
    "                # move toward the target object\n",
    "                found_object = True\n",
    "                print(\"found target object\")\n",
    "                center = detection_center(det)\n",
    "                center_offset = center[0]\n",
    "        \n",
    "                break\n",
    "        \n",
    "        if(found_object == True):\n",
    "            on_center = center_offset <= 0.125 and center_offset >= -0.125\n",
    "            \n",
    "            while(on_center == False):\n",
    "                print(\"center_offset \" + str(center_offset))\n",
    "                if center_offset > 0.125:\n",
    "                    print(\"on right\")\n",
    "                    robot.right(0.28)\n",
    "                    time.sleep(0.1)\n",
    "                    robot.stop()\n",
    "                    time.sleep(1)\n",
    "                elif center_offset < -0.125:\n",
    "                    print(\"on left\")\n",
    "                    robot.left(0.28)\n",
    "                    time.sleep(0.1)\n",
    "                    robot.stop()\n",
    "                    time.sleep(1)\n",
    "                    \n",
    "                image = camera.value\n",
    "                detections = model(image)\n",
    "                visualize_detections(image, detections[0]) \n",
    "                for det in detections[0]:\n",
    "                    bbox = det['bbox']\n",
    "                    label = det['label']\n",
    "                    if(label == target_obj_tall or label == target_obj_short):\n",
    "                        center = detection_center(det)\n",
    "                        on_center = center[0] < 0.125 and center[0] > -0.125\n",
    "                        center_offset = center[0]\n",
    "                        break\n",
    "\n",
    "            if(on_center == True):\n",
    "                is_moving_to_target_in_progress = True\n",
    "                robot.set_motors(0.3, 0.31)\n",
    "            \n",
    "        \n",
    "        if(found_object == False):\n",
    "            robot.left(0.28)\n",
    "            time.sleep(0.2)\n",
    "            robot.stop()\n",
    "            time.sleep(2)\n",
    "\n",
    "    print(\"end locking target\")\n",
    "        \n",
    "    is_locking_target_in_progress = False\n",
    "        \n",
    "\n",
    "\n",
    "def is_target_reached(detections):\n",
    "    # check if the car is close enough to the target\n",
    "    for det in detections:\n",
    "        bbox = det['bbox']\n",
    "        label = det['label']\n",
    "        if(label == target_obj_tall or label == target_obj_short):\n",
    "            dist = estimate_distance(det)\n",
    "            return dist < 20\n",
    "            \n",
    "    return False\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets.widgets as widgets\n",
    "image_widget = widgets.Image(format='jpeg', width=300, height=300)\n",
    "\n",
    "display(widgets.VBox([ image_widget]))\n",
    "\n",
    "def execute(change):\n",
    "\n",
    "    if(is_local_steering_in_progress == True or is_locking_target_in_progress == True):\n",
    "        return\n",
    "    \n",
    "    image = change['new']\n",
    "    \n",
    "    # compute all detected objects\n",
    "    detections = model(image)\n",
    "\n",
    "    \n",
    "    # visualized target and obstacle objects\n",
    "    visualize_detections(image, detections[0])\n",
    "\n",
    "    if(is_target_reached(detections[0])):\n",
    "        print(\"target reached!!!\")\n",
    "        camera.unobserve_all()\n",
    "        robot.stop()\n",
    "        return\n",
    "    \n",
    "    if(is_local_steering_in_progress):\n",
    "        print(\"is_local_steering_in_progress\")\n",
    "        return\n",
    "    \n",
    "    if(is_locking_target_in_progress):\n",
    "        print(\"is_locking_target_in_progress\")\n",
    "        return\n",
    "    \n",
    "\n",
    "    image_widget.value = bgr8_to_jpeg(image)\n",
    "    local_steering(detections[0])\n",
    "    \n",
    "    if(is_local_steering_in_progress == False and is_locking_target_in_progress == False and is_moving_to_target_in_progress == False):\n",
    "        camera.unobserve_all()\n",
    "        camera.observe(execute, names='value')\n",
    "        lock_and_move_to_target(detections[0])\n",
    "    \n",
    "\n",
    "    \n",
    "execute({'new': camera.value})\n",
    "\n",
    "camera.unobserve_all()\n",
    "camera.observe(execute, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot.stop()\n",
    "camera.unobserve_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
